import sklearn as sk
import numpy as np
import pandas as pd
import nltk
nltk.download('stopwords', 'word_tokenize')
from sklearn.feature_extraction import text
from sklearn.naive_bayes import MultinomialNB
#from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stops
#convec = sklearn.feature_extraction.text.CountVectorizer()

#Load the data
trainData = pd.read_csv("lab_train.txt")
testData = pd.read_csv("lab_test.txt")

#print(trainData)

#rainData2 = pd.DataFrame(trainData)
#trainData2.columns = ["num", "review", "rank"]
#tokenizer = nltk.RegexpTokenizer('[0-9]{7},', gaps=True)
#comments = tokenizer.tokenize(trainData)
#print(comments)

#Vectorizing and removing stopwords
vectorizer = text.TfidfVectorizer(use_idf=True, lowercase=True, strip_accents='ascii', stop_words='english')
#trainData2['review'].apply(nltk.word_tokenize)

vectorizedData = vectorizer.fit_transform(trainData.review)
vectorizedData.shape
#print(vectorizedData)

#Labels
labels=[]
for d in trainData.score:
    if(d < 3):
        labels.append('negative')
    else:
        labels.append('positive')


Bayes = MultinomialNB().fit(vectorizedData, labels)

newthing = ['"I hate this movie, it is about a lot of really bad things"'] #should be classified as negative, but is not
newthing2 = ['"The best movie I have ever seen, so important, happy and awesome, great and magnificent and wow cool."']
newvec = vectorizer.transform(newthing)
newvec2 = vectorizer.transform(newthing2)

print(Bayes.predict(newvec))
print(Bayes.predict(newvec2))



